{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e5b98c",
   "metadata": {},
   "source": [
    "# HackingGPT\n",
    "## Part 7: Building a Complete GPT from Scratch\n",
    "\n",
    "Part 7 brings everything together to build a complete GPT language model. We implement the full transformer architecture with:\n",
    "\n",
    "- **Multi-head self-attention** (the core mechanism)\n",
    "- **Feed-forward networks** with GELU activation\n",
    "- **Residual connections** and **layer normalization**\n",
    "- **AdamW optimizer** with **cosine learning rate scheduling**\n",
    "- **Gradient clipping** and **dropout** for stable training\n",
    "- **Early stopping with patience** to prevent overfitting and save compute time\n",
    "- **Model saving/loading** for inference\n",
    "\n",
    "By the end, you'll have a working character-level language model trained on Sherlock Holmes!\n",
    "\n",
    "#### Author: [Kevin Thomas](mailto:ket189@pitt.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e479a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce998e",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect the Data\n",
    "\n",
    "Our GPT learns from a text file (Sherlock Holmes stories). Key things we'll explore:\n",
    "\n",
    "- **Dataset size**: ~580K characters - enough to learn English patterns\n",
    "- **Vocabulary**: All unique characters (letters, numbers, punctuation)\n",
    "- **Character-level tokenization**: Each character = one token (simple but effective)\n",
    "\n",
    "We'll create mappings between characters and integers so the model can process text as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32287283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bf3e510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064ba054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the input text file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175d2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset statistics\n",
      "\n",
      "total characters: 581,565\n",
      "first 200 characters:\n",
      "﻿The Project Gutenberg eBook of The Adventures of Sherlock Holmes\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no\n"
     ]
    }
   ],
   "source": [
    "# display basic statistics about the text\n",
    "print('dataset statistics')\n",
    "print()\n",
    "print(f'total characters: {len(text):,}')\n",
    "print(f'first 200 characters:')\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c31309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '£',\n",
       " '½',\n",
       " 'à',\n",
       " 'â',\n",
       " 'æ',\n",
       " 'è',\n",
       " 'é',\n",
       " 'œ',\n",
       " '—',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '•',\n",
       " '™',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique characters in the text (our vocabulary)\n",
    "chars = sorted(list(set(text)))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1d12ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_size: number of unique characters\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d1e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary\n",
      "\n",
      "vocab_size = 98\n",
      "characters: '\\n !#$%&()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz£½àâæèéœ—‘’“”•™\\ufeff'\n"
     ]
    }
   ],
   "source": [
    "# display the vocabulary\n",
    "print('vocabulary')\n",
    "print()\n",
    "print(f'vocab_size = {vocab_size}')\n",
    "print(f'characters: {repr(\"\".join(chars))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084fd283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '#': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '/': 13,\n",
       " '0': 14,\n",
       " '1': 15,\n",
       " '2': 16,\n",
       " '3': 17,\n",
       " '4': 18,\n",
       " '5': 19,\n",
       " '6': 20,\n",
       " '7': 21,\n",
       " '8': 22,\n",
       " '9': 23,\n",
       " ':': 24,\n",
       " ';': 25,\n",
       " '?': 26,\n",
       " 'A': 27,\n",
       " 'B': 28,\n",
       " 'C': 29,\n",
       " 'D': 30,\n",
       " 'E': 31,\n",
       " 'F': 32,\n",
       " 'G': 33,\n",
       " 'H': 34,\n",
       " 'I': 35,\n",
       " 'J': 36,\n",
       " 'K': 37,\n",
       " 'L': 38,\n",
       " 'M': 39,\n",
       " 'N': 40,\n",
       " 'O': 41,\n",
       " 'P': 42,\n",
       " 'Q': 43,\n",
       " 'R': 44,\n",
       " 'S': 45,\n",
       " 'T': 46,\n",
       " 'U': 47,\n",
       " 'V': 48,\n",
       " 'W': 49,\n",
       " 'X': 50,\n",
       " 'Y': 51,\n",
       " 'Z': 52,\n",
       " '[': 53,\n",
       " ']': 54,\n",
       " '_': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '£': 82,\n",
       " '½': 83,\n",
       " 'à': 84,\n",
       " 'â': 85,\n",
       " 'æ': 86,\n",
       " 'è': 87,\n",
       " 'é': 88,\n",
       " 'œ': 89,\n",
       " '—': 90,\n",
       " '‘': 91,\n",
       " '’': 92,\n",
       " '“': 93,\n",
       " '”': 94,\n",
       " '•': 95,\n",
       " '™': 96,\n",
       " '\\ufeff': 97}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create character to integer mapping\n",
    "# stoi = \"string to integer\"\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094775f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '#',\n",
       " 4: '$',\n",
       " 5: '%',\n",
       " 6: '&',\n",
       " 7: '(',\n",
       " 8: ')',\n",
       " 9: '*',\n",
       " 10: ',',\n",
       " 11: '-',\n",
       " 12: '.',\n",
       " 13: '/',\n",
       " 14: '0',\n",
       " 15: '1',\n",
       " 16: '2',\n",
       " 17: '3',\n",
       " 18: '4',\n",
       " 19: '5',\n",
       " 20: '6',\n",
       " 21: '7',\n",
       " 22: '8',\n",
       " 23: '9',\n",
       " 24: ':',\n",
       " 25: ';',\n",
       " 26: '?',\n",
       " 27: 'A',\n",
       " 28: 'B',\n",
       " 29: 'C',\n",
       " 30: 'D',\n",
       " 31: 'E',\n",
       " 32: 'F',\n",
       " 33: 'G',\n",
       " 34: 'H',\n",
       " 35: 'I',\n",
       " 36: 'J',\n",
       " 37: 'K',\n",
       " 38: 'L',\n",
       " 39: 'M',\n",
       " 40: 'N',\n",
       " 41: 'O',\n",
       " 42: 'P',\n",
       " 43: 'Q',\n",
       " 44: 'R',\n",
       " 45: 'S',\n",
       " 46: 'T',\n",
       " 47: 'U',\n",
       " 48: 'V',\n",
       " 49: 'W',\n",
       " 50: 'X',\n",
       " 51: 'Y',\n",
       " 52: 'Z',\n",
       " 53: '[',\n",
       " 54: ']',\n",
       " 55: '_',\n",
       " 56: 'a',\n",
       " 57: 'b',\n",
       " 58: 'c',\n",
       " 59: 'd',\n",
       " 60: 'e',\n",
       " 61: 'f',\n",
       " 62: 'g',\n",
       " 63: 'h',\n",
       " 64: 'i',\n",
       " 65: 'j',\n",
       " 66: 'k',\n",
       " 67: 'l',\n",
       " 68: 'm',\n",
       " 69: 'n',\n",
       " 70: 'o',\n",
       " 71: 'p',\n",
       " 72: 'q',\n",
       " 73: 'r',\n",
       " 74: 's',\n",
       " 75: 't',\n",
       " 76: 'u',\n",
       " 77: 'v',\n",
       " 78: 'w',\n",
       " 79: 'x',\n",
       " 80: 'y',\n",
       " 81: 'z',\n",
       " 82: '£',\n",
       " 83: '½',\n",
       " 84: 'à',\n",
       " 85: 'â',\n",
       " 86: 'æ',\n",
       " 87: 'è',\n",
       " 88: 'é',\n",
       " 89: 'œ',\n",
       " 90: '—',\n",
       " 91: '‘',\n",
       " 92: '’',\n",
       " 93: '“',\n",
       " 94: '”',\n",
       " 95: '•',\n",
       " 96: '™',\n",
       " 97: '\\ufeff'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create integer to character mapping\n",
    "# itos = \"integer to string\"\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5968085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode function: convert string to list of integers\n",
    "def encode(s):\n",
    "    '''\n",
    "    Convert a string to a list of integers.\n",
    "    \n",
    "    Args:\n",
    "        s: input string to encode\n",
    "        \n",
    "    Returns:\n",
    "        list of integers representing each character\n",
    "    '''\n",
    "    return [stoi[c] for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71c9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode function: convert list of integers back to string\n",
    "def decode(l):\n",
    "    '''\n",
    "    Convert a list of integers back to a string.\n",
    "    \n",
    "    Args:\n",
    "        l: list of integers to decode\n",
    "        \n",
    "    Returns:\n",
    "        string representation of the integers\n",
    "    '''\n",
    "    return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b53f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing encode and decode\n",
      "\n",
      "original: 'hello'\n",
      "encoded: [63, 60, 67, 67, 70]\n",
      "decoded: 'hello'\n",
      "match: True\n"
     ]
    }
   ],
   "source": [
    "# test encode and decode\n",
    "print('testing encode and decode')\n",
    "print()\n",
    "test_string = 'hello'\n",
    "encoded = encode(test_string)\n",
    "decoded = decode(encoded)\n",
    "print(f'original: {repr(test_string)}')\n",
    "print(f'encoded: {encoded}')\n",
    "print(f'decoded: {repr(decoded)}')\n",
    "print(f'match: {test_string == decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81b189c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([97, 46, 63,  ...,  0,  0,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the entire text into a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931e2887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded data tensor\n",
      "\n",
      "shape: torch.Size([581565])\n",
      "dtype: torch.int64\n",
      "first 100 tokens: [97, 46, 63, 60, 1, 42, 73, 70, 65, 60, 58, 75, 1, 33, 76, 75, 60, 69, 57, 60, 73, 62, 1, 60, 28, 70, 70, 66, 1, 70, 61, 1, 46, 63, 60, 1, 27, 59, 77, 60, 69, 75, 76, 73, 60, 74, 1, 70, 61, 1, 45, 63, 60, 73, 67, 70, 58, 66, 1, 34, 70, 67, 68, 60, 74, 0, 1, 1, 1, 1, 0, 46, 63, 64, 74, 1, 60, 57, 70, 70, 66, 1, 64, 74, 1, 61, 70, 73, 1, 75, 63, 60, 1, 76, 74, 60, 1, 70, 61, 1]\n"
     ]
    }
   ],
   "source": [
    "# display data tensor info\n",
    "print('encoded data tensor')\n",
    "print()\n",
    "print(f'shape: {data.shape}')\n",
    "print(f'dtype: {data.dtype}')\n",
    "print(f'first 100 tokens: {data[:100].tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498eccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "# 90% for training, 10% for validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79daef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation split\n",
      "\n",
      "total tokens: 581,565\n",
      "train tokens: 523,408 (90.0%)\n",
      "val tokens: 58,157 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# display split information\n",
    "print('train/validation split')\n",
    "print()\n",
    "print(f'total tokens: {len(data):,}')\n",
    "print(f'train tokens: {len(train_data):,} ({100*len(train_data)/len(data):.1f}%)')\n",
    "print(f'val tokens: {len(val_data):,} ({100*len(val_data)/len(data):.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b6c28",
   "metadata": {},
   "source": [
    "## Step 2: Define All Hyperparameters\n",
    "\n",
    "Hyperparameters control the model's architecture and training behavior. Unlike model weights (which are learned), hyperparameters are set by us before training.\n",
    "\n",
    "**Architecture hyperparameters** determine model size:\n",
    "- `n_embd`, `n_head`, `n_layer` → bigger = more capacity but slower\n",
    "\n",
    "**Training hyperparameters** control how the model learns:\n",
    "- `batch_size`, `learning_rate`, `max_iters` → affect convergence speed and quality\n",
    "\n",
    "**Regularization hyperparameters** prevent overfitting:\n",
    "- `dropout` → randomly drops connections during training to improve generalization\n",
    "\n",
    "**Early stopping hyperparameters** save compute time:\n",
    "- `patience` → number of evaluations without improvement before stopping training early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c47e9262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size: how many independent sequences to process in parallel\n",
    "# larger = faster training but more memory\n",
    "batch_size = 64\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6061d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block_size: maximum context length for predictions\n",
    "# this is how many tokens the model can \"see\" when making a prediction\n",
    "block_size = 256\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c926f1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_iters: total number of training iterations\n",
    "max_iters = 10000\n",
    "max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "374a9e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval_interval: how often to evaluate loss on train/val sets\n",
    "eval_interval = 500\n",
    "eval_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd4a04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate: step size for optimizer\n",
    "# too high = unstable, too low = slow learning\n",
    "learning_rate = 5e-4\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4722823f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device: use GPU if available for faster training\n",
    "# priority: CUDA (NVIDIA) > MPS (Apple Silicon) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    # NVIDIA GPU (Windows/Linux)\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Apple Silicon GPU (Mac M1/M2/M3)\n",
    "    device = 'mps'\n",
    "else:\n",
    "    # fallback to CPU\n",
    "    device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3baa7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval_iters: how many batches to average over when estimating loss\n",
    "eval_iters = 200\n",
    "eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "213b3757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_embd: embedding dimension (size of token representations)\n",
    "# larger = more expressive but more parameters\n",
    "n_embd = 384\n",
    "n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40af1a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_head: number of attention heads in multi-head attention\n",
    "# each head learns different patterns\n",
    "n_head = 6\n",
    "n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a78e3971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_layer: number of transformer blocks stacked\n",
    "# deeper = more complex patterns but harder to train\n",
    "n_layer = 6\n",
    "n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a161de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropout: probability of dropping units during training\n",
    "# helps prevent overfitting\n",
    "dropout = 0.35\n",
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ee2722f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patience: number of evaluations to wait for improvement before stopping\n",
    "# if validation loss doesn't improve for this many checks, training stops early\n",
    "# with eval_interval=500 and patience=5, stops after 2500 steps of no improvement\n",
    "patience = 5\n",
    "patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b992e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter summary\n",
      "\n",
      "batch_size = 64\n",
      "block_size = 256\n",
      "max_iters = 10000\n",
      "eval_interval = 500\n",
      "learning_rate = 0.0005\n",
      "eval_iters = 200\n",
      "n_embd = 384\n",
      "n_head = 6\n",
      "n_layer = 6\n",
      "dropout = 0.35\n",
      "patience = 5\n",
      "\n",
      "derived values\n",
      "head_size = n_embd // n_head = 384 // 6 = 64\n",
      "\n",
      "device information\n",
      "device = mps\n",
      "   GPU: Apple Silicon (MPS)\n"
     ]
    }
   ],
   "source": [
    "# display all hyperparameters together\n",
    "print('hyperparameter summary')\n",
    "print()\n",
    "print(f'batch_size = {batch_size}')\n",
    "print(f'block_size = {block_size}')\n",
    "print(f'max_iters = {max_iters}')\n",
    "print(f'eval_interval = {eval_interval}')\n",
    "print(f'learning_rate = {learning_rate}')\n",
    "print(f'eval_iters = {eval_iters}')\n",
    "print(f'n_embd = {n_embd}')\n",
    "print(f'n_head = {n_head}')\n",
    "print(f'n_layer = {n_layer}')\n",
    "print(f'dropout = {dropout}')\n",
    "print(f'patience = {patience}')\n",
    "print()\n",
    "print('derived values')\n",
    "print(f'head_size = n_embd // n_head = {n_embd} // {n_head} = {n_embd // n_head}')\n",
    "print()\n",
    "print('device information')\n",
    "print(f'device = {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'   CUDA version: {torch.version.cuda}')\n",
    "elif device == 'mps':\n",
    "    print('   GPU: Apple Silicon (MPS)')\n",
    "else:\n",
    "    print('   using CPU (no GPU acceleration)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823c3ac",
   "metadata": {},
   "source": [
    "## Step 3: Create Batch Generator\n",
    "\n",
    "Training on one example at a time is inefficient. We batch multiple sequences together for the following.\n",
    "- **GPU parallelism**: Process many sequences simultaneously\n",
    "- **Stable gradients**: Averaging over a batch reduces noise\n",
    "- **Faster training**: More data processed per forward/backward pass\n",
    "\n",
    "Each batch contains `batch_size` independent sequences of length `block_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7d9baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch: generate a batch of training examples\n",
    "def get_batch(split):\n",
    "    '''\n",
    "    Generate a batch of input-target pairs for training or validation.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val' to select which dataset to use\n",
    "        \n",
    "    Returns:\n",
    "        x: input tensor of shape (batch_size, block_size)\n",
    "        y: target tensor of shape (batch_size, block_size)\n",
    "    '''\n",
    "    # select the appropriate dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # generate random starting positions for each sequence in the batch\n",
    "    # we need room for block_size tokens, so max start is len(data) - block_size\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # stack the sequences into batches\n",
    "    # x[i] = data[ix[i] : ix[i] + block_size]\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # y is shifted by 1: y[i] = data[ix[i] + 1 : ix[i] + block_size + 1]\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # move to device (GPU if available)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # return the input-target pair\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e889935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing get_batch\n",
      "\n",
      "x shape: torch.Size([64, 256])\n",
      "y shape: torch.Size([64, 256])\n",
      "\n",
      "first sequence in batch\n",
      "x[0]: [1, 59, 64, 73, 60, 58, 75, 70, 73, 74]... (first 10 tokens)\n",
      "y[0]: [59, 64, 73, 60, 58, 75, 70, 73, 74, 1]... (first 10 tokens)\n",
      "\n",
      "note: y is x shifted by 1 position\n",
      "x[0][1] = 59 should equal y[0][0] = 59\n"
     ]
    }
   ],
   "source": [
    "# test get_batch\n",
    "xb, yb = get_batch('train')\n",
    "print('testing get_batch')\n",
    "print()\n",
    "print(f'x shape: {xb.shape}')\n",
    "print(f'y shape: {yb.shape}')\n",
    "print()\n",
    "print('first sequence in batch')\n",
    "print(f'x[0]: {xb[0].tolist()[:10]}... (first 10 tokens)')\n",
    "print(f'y[0]: {yb[0].tolist()[:10]}... (first 10 tokens)')\n",
    "print()\n",
    "print('note: y is x shifted by 1 position')\n",
    "print(f'x[0][1] = {xb[0][1].item()} should equal y[0][0] = {yb[0][0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a3c6e",
   "metadata": {},
   "source": [
    "## Step 4: Loss Estimation Function\n",
    "\n",
    "We need a reliable way to measure model performance during training. Rather than using a single batch (which is noisy), we average loss over many batches for a stable estimate.\n",
    "\n",
    "**Why average over multiple batches?**\n",
    "- Single batch loss fluctuates randomly\n",
    "- Averaging gives smoother, more reliable signal\n",
    "- We evaluate on both train and validation sets to detect overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7142b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate_loss: average loss over multiple batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    '''\n",
    "    Estimate loss by averaging over eval_iters batches.\n",
    "    \n",
    "    Uses @torch.no_grad() decorator to disable gradient computation\n",
    "    since we are only evaluating, not training.\n",
    "    \n",
    "    Args:\n",
    "        model: the model to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'train' and 'val' average losses\n",
    "    '''\n",
    "    # initialize output dictionary for train and val losses\n",
    "    out = {}\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate over train and val splits\n",
    "    for split in ['train', 'val']:\n",
    "        # create tensor to store losses for averaging\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        # sample eval_iters batches and compute loss for each\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        # store mean loss for this split\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # set model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # return dictionary with train and val losses\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322142b2",
   "metadata": {},
   "source": [
    "## Step 5: Single Attention Head\n",
    "\n",
    "Self-attention is the core mechanism of transformers. It allows each token to \"look at\" all previous tokens and decide which ones are relevant.\n",
    "\n",
    "**The Key Concepts:**\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\"  \n",
    "- **Value (V)**: \"What information do I provide?\"\n",
    "\n",
    "**How it works:**\n",
    "1. Each token creates Q, K, V vectors via learned projections\n",
    "2. Attention scores = Q @ K^T (how much does each token match?)\n",
    "3. Scale by √(head_size) to keep gradients stable\n",
    "4. Apply causal mask (can't attend to future tokens)\n",
    "5. Softmax to get attention weights (sum to 1)\n",
    "6. Weighted sum of V vectors = output\n",
    "\n",
    "**Causal masking** is crucial for autoregressive generation: each position can only attend to earlier positions (no peeking at the future!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13a06ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head: one head of self-attention\n",
    "class Head(nn.Module):\n",
    "    '''\n",
    "    Single head of self-attention.\n",
    "    \n",
    "    Computes scaled dot-product attention:\n",
    "        Attention(Q, K, V) = softmax(Q @ K^T / sqrt(head_size)) @ V\n",
    "    \n",
    "    Args:\n",
    "        head_size: dimension of queries, keys, and values\n",
    "    \n",
    "    Attributes:\n",
    "        key: linear projection for keys (what I contain)\n",
    "        query: linear projection for queries (what I'm looking for)\n",
    "        value: linear projection for values (what I'll give)\n",
    "        tril: lower triangular mask for causal attention\n",
    "        dropout: dropout layer for regularization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        '''\n",
    "        Initialize the attention head.\n",
    "        \n",
    "        Args:\n",
    "            head_size: dimension of the attention head\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # key projection: (n_embd) -> (head_size)\n",
    "        # \"what information do I contain?\"\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # query projection: (n_embd) -> (head_size)\n",
    "        # \"what information am I looking for?\"\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # value projection: (n_embd) -> (head_size)\n",
    "        # \"what information will I provide if attended to?\"\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # register_buffer: not a parameter, but should be saved with model\n",
    "        # this is the causal mask (lower triangular matrix)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        # type hint for tril buffer\n",
    "        self.tril: torch.Tensor\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the attention head.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "               B = batch size\n",
    "               T = sequence length\n",
    "               C = n_embd (embedding dimension)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, head_size)\n",
    "        '''\n",
    "        # get dimensions (only T is needed for masking)\n",
    "        _, T, _ = x.shape\n",
    "        \n",
    "        # compute keys: (B, T, C) -> (B, T, head_size)\n",
    "        k = self.key(x)\n",
    "        \n",
    "        # compute queries: (B, T, C) -> (B, T, head_size)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # compute attention scores (affinities)\n",
    "        # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        # scale by 1/sqrt(head_size) to prevent softmax from becoming too peaky\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        \n",
    "        # apply causal mask: positions can only attend to previous positions\n",
    "        # mask future positions with -inf so softmax gives them 0 weight\n",
    "        tril = self.tril\n",
    "        wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # apply softmax to get attention weights (probabilities)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        # apply dropout\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # compute values: (B, T, C) -> (B, T, head_size)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # weighted aggregation of values\n",
    "        # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        out = wei @ v\n",
    "        \n",
    "        # return the attention output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac8b4b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Head class\n",
      "\n",
      "head_size = n_embd // n_head = 384 // 6 = 64\n",
      "\n",
      "key projection: Linear(in_features=384, out_features=64, bias=False)\n",
      "query projection: Linear(in_features=384, out_features=64, bias=False)\n",
      "value projection: Linear(in_features=384, out_features=64, bias=False)\n",
      "\n",
      "causal mask shape: torch.Size([256, 256])\n",
      "first 4x4 of causal mask:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# test Head class\n",
    "print('testing Head class')\n",
    "print()\n",
    "head_size = n_embd // n_head\n",
    "print(f'head_size = n_embd // n_head = {n_embd} // {n_head} = {head_size}')\n",
    "print()\n",
    "test_head = Head(head_size)\n",
    "print(f'key projection: {test_head.key}')\n",
    "print(f'query projection: {test_head.query}')\n",
    "print(f'value projection: {test_head.value}')\n",
    "print()\n",
    "print(f'causal mask shape: {test_head.tril.shape}')\n",
    "print(f'first 4x4 of causal mask:')\n",
    "tril_tensor = test_head.tril\n",
    "print(tril_tensor[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5864d5e",
   "metadata": {},
   "source": [
    "## Step 6: Multi-Head Attention\n",
    "\n",
    "A single attention head has limited expressive power. Multi-head attention runs **multiple heads in parallel**, allowing the model to attend to different patterns simultaneously.\n",
    "\n",
    "**Why multiple heads?**\n",
    "- One head might focus on nearby tokens (local patterns)\n",
    "- Another might focus on syntactic relationships\n",
    "- Another might track semantic meaning\n",
    "- Different heads specialize for different tasks\n",
    "\n",
    "**How it works:**\n",
    "1. Run `n_head` independent attention operations in parallel\n",
    "2. Concatenate all outputs along the feature dimension\n",
    "3. Apply a final linear projection to mix information across heads\n",
    "\n",
    "With `n_head=6` and `head_size=64`, we get 6 different \"perspectives\" on the relationships in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221e5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention: multiple heads of self-attention in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Multi-head self-attention.\n",
    "    \n",
    "    Runs multiple attention heads in parallel and concatenates the results.\n",
    "    Then projects back to the embedding dimension.\n",
    "    \n",
    "    The idea is that each head can learn to attend to different things:\n",
    "    - One head might attend to syntax\n",
    "    - Another might attend to semantics\n",
    "    - Another might attend to position patterns\n",
    "    \n",
    "    Args:\n",
    "        num_heads: number of attention heads\n",
    "        head_size: dimension of each head\n",
    "    \n",
    "    Attributes:\n",
    "        heads: ModuleList of Head modules\n",
    "        proj: output projection back to n_embd\n",
    "        dropout: dropout layer for regularization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        '''\n",
    "        Initialize multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            num_heads: number of parallel attention heads\n",
    "            head_size: dimension of each attention head\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # create num_heads attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "        # output projection: (num_heads * head_size) -> (n_embd)\n",
    "        # this is the Wo matrix in the paper\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, C)\n",
    "        '''\n",
    "        # run all heads in parallel and concatenate along the last dimension\n",
    "        # each head outputs (B, T, head_size)\n",
    "        # concatenation gives (B, T, num_heads * head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        # project back to embedding dimension and apply dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        \n",
    "        # return the multi-head attention output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b4fbd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing MultiHeadAttention\n",
      "\n",
      "n_head = 6\n",
      "head_size = 64\n",
      "n_head * head_size = 384 = n_embd = 384\n",
      "\n",
      "number of heads: 6\n",
      "output projection: Linear(in_features=384, out_features=384, bias=True)\n",
      "\n",
      "shape flow:\n",
      "   input: (B, T, 384)\n",
      "   each head: (B, T, 64)\n",
      "   concat: (B, T, 6 * 64) = (B, T, 384)\n",
      "   proj: (B, T, 384)\n"
     ]
    }
   ],
   "source": [
    "# test MultiHeadAttention\n",
    "print('testing MultiHeadAttention')\n",
    "print()\n",
    "head_size = n_embd // n_head\n",
    "print(f'n_head = {n_head}')\n",
    "print(f'head_size = {head_size}')\n",
    "print(f'n_head * head_size = {n_head * head_size} = n_embd = {n_embd}')\n",
    "print()\n",
    "test_mha = MultiHeadAttention(n_head, head_size)\n",
    "print(f'number of heads: {len(test_mha.heads)}')\n",
    "print(f'output projection: {test_mha.proj}')\n",
    "print()\n",
    "print('shape flow:')\n",
    "print(f'   input: (B, T, {n_embd})')\n",
    "print(f'   each head: (B, T, {head_size})')\n",
    "print(f'   concat: (B, T, {n_head} * {head_size}) = (B, T, {n_head * head_size})')\n",
    "print(f'   proj: (B, T, {n_embd})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6e98d",
   "metadata": {},
   "source": [
    "## Step 7: Feed-Forward Network\n",
    "\n",
    "After attention, each token is processed independently by a small neural network. This is where the model does its \"thinking\" on each position.\n",
    "\n",
    "**Architecture**: Linear → GELU → Linear → Dropout\n",
    "- First linear layer **expands** from `n_embd` to `4 × n_embd`\n",
    "- GELU activation introduces non-linearity\n",
    "- Second linear layer **compresses** back to `n_embd`\n",
    "\n",
    "**Why GELU instead of ReLU?**\n",
    "- GELU (Gaussian Error Linear Unit) is smoother than ReLU\n",
    "- Used in GPT-2, GPT-3, and BERT\n",
    "- ReLU has a hard cutoff at 0, GELU has a smooth curve\n",
    "- This helps gradients flow better during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d2a5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward: simple feed-forward network (per token)\n",
    "class FeedForward(nn.Module):\n",
    "    '''\n",
    "    Position-wise feed-forward network.\n",
    "    \n",
    "    Applied to each position independently and identically.\n",
    "    Expands to 4x the embedding dimension, applies GELU, then projects back.\n",
    "    \n",
    "    FFN(x) = GELU(x @ W1 + b1) @ W2 + b2\n",
    "    \n",
    "    The 4x expansion allows the model to have a larger intermediate\n",
    "    representation for \"thinking\" before compressing back down.\n",
    "    \n",
    "    Args:\n",
    "        n_embd: embedding dimension\n",
    "    \n",
    "    Attributes:\n",
    "        net: Sequential network with linear, gelu, linear, dropout\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        '''\n",
    "        Initialize the feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            n_embd: embedding dimension (input and output size)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # build the feed-forward network\n",
    "        self.net = nn.Sequential(\n",
    "            # expand to 4x embedding dimension\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            # GELU non-linearity (smoother than ReLU, used in GPT-2/3)\n",
    "            nn.GELU(),\n",
    "            # project back to embedding dimension\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            # dropout for regularization\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, C)\n",
    "        '''\n",
    "        # apply the feed-forward network and return output\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0c5ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing FeedForward\n",
      "\n",
      "input dimension: 384\n",
      "hidden dimension: 1536\n",
      "output dimension: 384\n",
      "\n",
      "network architecture:\n",
      "   layer 0: Linear(in_features=384, out_features=1536, bias=True)\n",
      "   layer 1: GELU(approximate='none')\n",
      "   layer 2: Linear(in_features=1536, out_features=384, bias=True)\n",
      "   layer 3: Dropout(p=0.35, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "# test FeedForward\n",
    "print('testing FeedForward')\n",
    "print()\n",
    "test_ffn = FeedForward(n_embd)\n",
    "print(f'input dimension: {n_embd}')\n",
    "print(f'hidden dimension: {4 * n_embd}')\n",
    "print(f'output dimension: {n_embd}')\n",
    "print()\n",
    "print('network architecture:')\n",
    "for i, layer in enumerate(list(test_ffn.net.children())):\n",
    "    print(f'   layer {i}: {layer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08721d",
   "metadata": {},
   "source": [
    "## Step 8: Transformer Block\n",
    "\n",
    "A transformer block is the fundamental building unit that we stack to create deep models. Each block has two main phases:\n",
    "\n",
    "**1. Communication (Attention)**\n",
    "- Tokens exchange information via multi-head attention\n",
    "- Each token learns about its context\n",
    "\n",
    "**2. Computation (Feed-Forward)**  \n",
    "- Each token processes its updated representation\n",
    "- Independent computation at each position\n",
    "\n",
    "**Key Design Choices:**\n",
    "- **Pre-norm architecture**: LayerNorm before (not after) each sub-layer → more stable training\n",
    "- **Residual connections**: `x = x + sublayer(x)` → gradients flow easily through deep networks\n",
    "\n",
    "The pattern repeats: `x → LayerNorm → Attention → +x → LayerNorm → FFN → +x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55edbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block: transformer block with communication followed by computation\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    Transformer block: communication (attention) followed by computation (ffn).\n",
    "    \n",
    "    Uses pre-norm architecture where layer norm is applied before each sub-layer.\n",
    "    Residual connections allow gradients to flow directly through the network.\n",
    "    \n",
    "    The structure is:\n",
    "        x = x + attention(ln1(x))  # communication\n",
    "        x = x + ffn(ln2(x))        # computation\n",
    "    \n",
    "    Args:\n",
    "        n_embd: embedding dimension\n",
    "        n_head: number of attention heads\n",
    "    \n",
    "    Attributes:\n",
    "        sa: self-attention module (multi-head attention)\n",
    "        ffwd: feed-forward network\n",
    "        ln1: first layer normalization\n",
    "        ln2: second layer normalization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        '''\n",
    "        Initialize the transformer block.\n",
    "        \n",
    "        Args:\n",
    "            n_embd: embedding dimension\n",
    "            n_head: number of attention heads\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # calculate head size: n_embd must be divisible by n_head\n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        # self-attention layer (communication)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        \n",
    "        # feed-forward layer (computation)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        \n",
    "        # layer normalizations (pre-norm architecture)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, C)\n",
    "        '''\n",
    "        # self-attention with residual connection\n",
    "        # x = x + attention(ln1(x))\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # feed-forward with residual connection\n",
    "        # x = x + ffn(ln2(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        # return the transformed output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e859b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Block\n",
      "\n",
      "n_embd = 384\n",
      "n_head = 6\n",
      "head_size = 64\n",
      "\n",
      "block components:\n",
      "   ln1: LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "   self-attention: MultiHeadAttention with 6 heads\n",
      "   ln2: LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "   ffwd: FeedForward with hidden dim 1536\n",
      "\n",
      "data flow:\n",
      "   x -> ln1 -> attention -> + residual -> ln2 -> ffn -> + residual -> out\n"
     ]
    }
   ],
   "source": [
    "# test Block\n",
    "print('testing Block')\n",
    "print()\n",
    "test_block = Block(n_embd, n_head)\n",
    "print(f'n_embd = {n_embd}')\n",
    "print(f'n_head = {n_head}')\n",
    "print(f'head_size = {n_embd // n_head}')\n",
    "print()\n",
    "print('block components:')\n",
    "print(f'   ln1: {test_block.ln1}')\n",
    "print(f'   self-attention: MultiHeadAttention with {n_head} heads')\n",
    "print(f'   ln2: {test_block.ln2}')\n",
    "print(f'   ffwd: FeedForward with hidden dim {4 * n_embd}')\n",
    "print()\n",
    "print('data flow:')\n",
    "print('   x -> ln1 -> attention -> + residual -> ln2 -> ffn -> + residual -> out')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251392f5",
   "metadata": {},
   "source": [
    "## Step 9: Complete GPT Language Model\n",
    "\n",
    "Now we assemble all the pieces into a complete GPT! The architecture stacks:\n",
    "\n",
    "1. **Token Embedding**: Converts token IDs → learned vectors\n",
    "2. **Position Embedding**: Adds position information (since attention has no inherent order)\n",
    "3. **N Transformer Blocks**: Stack of attention + FFN (we use 6 blocks)\n",
    "4. **Final LayerNorm**: Stabilizes activations before output\n",
    "5. **Language Model Head**: Projects to vocabulary size for next-token prediction\n",
    "\n",
    "The model also includes:\n",
    "- **Weight initialization**: Normal distribution with std=0.02 (helps training)\n",
    "- **Generate method**: Autoregressive text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e0df0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTLanguageModel: the complete GPT model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    '''\n",
    "    GPT Language Model.\n",
    "    \n",
    "    A decoder-only transformer that predicts the next token given previous tokens.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token embedding: convert token indices to vectors\n",
    "        2. Position embedding: add position information\n",
    "        3. Transformer blocks: stack of attention + ffn blocks\n",
    "        4. Final layer norm: normalize before output\n",
    "        5. Language model head: project to vocabulary size for predictions\n",
    "    \n",
    "    Attributes:\n",
    "        token_embedding_table: lookup table for token embeddings\n",
    "        position_embedding_table: lookup table for position embeddings\n",
    "        blocks: Sequential stack of transformer blocks\n",
    "        ln_f: final layer normalization\n",
    "        lm_head: linear projection to vocabulary size\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the GPT language model.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding table: (vocab_size, n_embd)\n",
    "        # each token gets a learned vector representation\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # position embedding table: (block_size, n_embd)\n",
    "        # each position gets a learned vector representation\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # stack of transformer blocks\n",
    "        # Sequential applies each block in order\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # language model head: project from n_embd to vocab_size\n",
    "        # this gives us logits (unnormalized probabilities) for each token\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        '''\n",
    "        Initialize weights for the model.\n",
    "        \n",
    "        Linear layers get normal initialization with std=0.02.\n",
    "        Embedding layers get normal initialization with std=0.02.\n",
    "        Biases are initialized to zero.\n",
    "        \n",
    "        Args:\n",
    "            module: the module to initialize\n",
    "        '''\n",
    "        # initialize linear layers\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        # initialize embedding layers\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Forward pass of the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            idx: input token indices of shape (B, T)\n",
    "            targets: target token indices of shape (B, T), optional\n",
    "        \n",
    "        Returns:\n",
    "            logits: unnormalized predictions of shape (B, T, vocab_size)\n",
    "            loss: cross-entropy loss if targets provided, else None\n",
    "        '''\n",
    "        # extract sequence length from input shape\n",
    "        _, T = idx.shape\n",
    "        \n",
    "        # get token embeddings: (B, T) -> (B, T, n_embd)\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        \n",
    "        # get position embeddings: (T,) -> (T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        # combine token and position embeddings: (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # pass through transformer blocks: (B, T, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # final layer norm: (B, T, n_embd)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # project to vocabulary: (B, T, n_embd) -> (B, T, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape for cross entropy: (B*T, vocab_size) and (B*T,)\n",
    "            _, _, C_logits = logits.shape\n",
    "            logits_flat = logits.view(-1, C_logits)\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        # return predictions and optional loss\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Given a context, generate max_new_tokens new tokens one at a time.\n",
    "        \n",
    "        Args:\n",
    "            idx: starting context of shape (B, T)\n",
    "            max_new_tokens: number of new tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            idx: extended sequence of shape (B, T + max_new_tokens)\n",
    "        '''\n",
    "        # generate one token at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop context to block_size (model can only handle block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # get predictions using forward method\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # focus on last time step: (B, vocab_size)\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # append to running sequence: (B, T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        # return the extended sequence\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9318c",
   "metadata": {},
   "source": [
    "## Step 10: Create and Analyze the Model\n",
    "\n",
    "Let's instantiate our GPT and examine its structure:\n",
    "\n",
    "- **Total parameters**: ~10.8M trainable weights\n",
    "- **Device**: Automatically uses GPU (CUDA/MPS) if available, otherwise CPU\n",
    "- **Memory**: Parameters stored in GPU memory for fast computation\n",
    "\n",
    "The parameter count breakdown helps understand where the model's capacity lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb19c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created and moved to mps\n"
     ]
    }
   ],
   "source": [
    "# create the model and move to device\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "print(f'model created and moved to {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e74147ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count breakdown\n",
      "\n",
      "token embeddings: 98 x 384 = 37,632\n",
      "position embeddings: 256 x 384 = 98,304\n",
      "\n",
      "each transformer block:\n",
      "   attention Q,K,V: 3 x 384 x 64 x 6 = 442,368\n",
      "   attention proj: 384 x 384 = 147,456\n",
      "   ffn expand: 384 x 1536 = 589,824\n",
      "   ffn contract: 1536 x 384 = 589,824\n",
      "   layer norms: 2 norms x (384 weights + 384 biases) = 1,536\n",
      "\n",
      "language model head: 384 x 98 = 37,632\n",
      "\n",
      "total parameters: 10,814,306 (10.81M)\n"
     ]
    }
   ],
   "source": [
    "# count model parameters\n",
    "total_params = sum(p.numel() for p in m.parameters())\n",
    "print('parameter count breakdown')\n",
    "print()\n",
    "print(f'token embeddings: {vocab_size} x {n_embd} = {vocab_size * n_embd:,}')\n",
    "print(f'position embeddings: {block_size} x {n_embd} = {block_size * n_embd:,}')\n",
    "print()\n",
    "print('each transformer block:')\n",
    "print(f'   attention Q,K,V: 3 x {n_embd} x {n_embd // n_head} x {n_head} = {3 * n_embd * n_embd:,}')\n",
    "print(f'   attention proj: {n_embd} x {n_embd} = {n_embd * n_embd:,}')\n",
    "print(f'   ffn expand: {n_embd} x {4 * n_embd} = {n_embd * 4 * n_embd:,}')\n",
    "print(f'   ffn contract: {4 * n_embd} x {n_embd} = {4 * n_embd * n_embd:,}')\n",
    "print(f'   layer norms: 2 norms x ({n_embd} weights + {n_embd} biases) = {4 * n_embd:,}')\n",
    "print()\n",
    "print(f'language model head: {n_embd} x {vocab_size} = {n_embd * vocab_size:,}')\n",
    "print()\n",
    "print(f'total parameters: {total_params:,} ({total_params/1e6:.2f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b70215",
   "metadata": {},
   "source": [
    "## Step 11: Training Loop with Early Stopping\n",
    "\n",
    "Now we train our GPT model using several advanced techniques:\n",
    "\n",
    "### Optimizer: AdamW\n",
    "- **What it is**: Adam optimizer with decoupled weight decay\n",
    "- **Why we use it**: Proper L2 regularization helps prevent overfitting\n",
    "- **Our setting**: `weight_decay=0.1` penalizes large weights\n",
    "\n",
    "### Learning Rate Scheduler: Cosine Annealing\n",
    "- **What it does**: Smoothly decreases learning rate following a cosine curve\n",
    "- **Why we use it**: High LR early for fast progress, low LR later for fine-tuning\n",
    "- **Our setting**: Decays from `1e-3` → `1e-5` over 10,000 iterations\n",
    "\n",
    "### Gradient Clipping\n",
    "- **What it does**: Limits the maximum gradient norm during backpropagation\n",
    "- **Why we use it**: Prevents exploding gradients that can destabilize training\n",
    "- **Our setting**: `max_norm=1.0` clips gradients if their norm exceeds 1.0\n",
    "\n",
    "### Early Stopping with Patience\n",
    "- **What it does**: Stops training when validation loss stops improving for `patience` evaluations\n",
    "- **Why we use it**: Training loss can keep decreasing while validation loss rises (overfitting)\n",
    "- **How patience works**: If val loss doesn't improve for `patience` consecutive checks, training stops immediately\n",
    "- **Our setting**: `patience=3` with `eval_interval=500` means stop after 1,500 steps of no improvement\n",
    "- **Best model restoration**: We always restore the best checkpoint, not the final (potentially overfit) model\n",
    "\n",
    "### How Early Stopping Works Step-by-Step\n",
    "1. Every `eval_interval` steps, we check validation loss\n",
    "2. If val loss improved → save model weights, reset patience counter to 0\n",
    "3. If val loss did NOT improve → increment patience counter by 1\n",
    "4. If patience counter reaches `patience` → stop training immediately\n",
    "5. After training ends → restore the saved best model weights\n",
    "\n",
    "**Example with our settings:**\n",
    "- Step 0: val loss 4.20 → best! save model, patience = 0\n",
    "- Step 500: val loss 2.10 → best! save model, patience = 0  \n",
    "- Step 1000: val loss 1.70 → best! save model, patience = 0\n",
    "- Step 1500: val loss 1.55 → best! save model, patience = 0\n",
    "- Step 2000: val loss 1.58 → worse, patience = 1/3\n",
    "- Step 2500: val loss 1.60 → worse, patience = 2/3\n",
    "- Step 3000: val loss 1.62 → worse, patience = 3/3 → STOP!\n",
    "- Restore model from step 1500 (val loss 1.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer: AdamW with learning rate 0.0005, weight decay 0.15\n",
      "scheduler: CosineAnnealingLR from 0.0005 to 1e-5 over 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "# AdamW is Adam with proper weight decay\n",
    "# weight_decay adds L2 regularization to prevent overfitting\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.15)\n",
    "\n",
    "# create learning rate scheduler\n",
    "# cosine annealing smoothly decays learning rate from max to min\n",
    "# this helps the model converge better in later stages of training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters, eta_min=1e-5)\n",
    "\n",
    "# display optimizer and scheduler configuration\n",
    "print(f'optimizer: AdamW with learning rate {learning_rate}, weight decay 0.15')\n",
    "print(f'scheduler: CosineAnnealingLR from {learning_rate} to 1e-5 over {max_iters} iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5296a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training GPT for up to 10000 iterations\n",
      "evaluating every 500 iterations\n",
      "early stopping patience: 5 evaluations (2500 steps)\n",
      "============================================================\n",
      "step     0: train loss 4.5899, val loss 4.5908, lr 5.00e-04 *best*\n",
      "step   500: train loss 1.8271, val loss 2.0141, lr 4.97e-04 *best*\n",
      "step  1000: train loss 1.3953, val loss 1.7005, lr 4.88e-04 *best*\n",
      "step  1500: train loss 1.1915, val loss 1.5746, lr 4.73e-04 *best*\n",
      "step  2000: train loss 1.0829, val loss 1.5397, lr 4.53e-04 *best*\n",
      "step  2500: train loss 0.9958, val loss 1.5423, lr 4.28e-04 (patience 1/5)\n",
      "step  3000: train loss 0.9194, val loss 1.5349, lr 3.99e-04 *best*\n",
      "step  3500: train loss 0.8458, val loss 1.5823, lr 3.66e-04 (patience 1/5)\n",
      "step  4000: train loss 0.7750, val loss 1.6280, lr 3.31e-04 (patience 2/5)\n",
      "step  4500: train loss 0.7059, val loss 1.6640, lr 2.93e-04 (patience 3/5)\n",
      "step  5000: train loss 0.6454, val loss 1.7080, lr 2.55e-04 (patience 4/5)\n",
      "step  5500: train loss 0.5815, val loss 1.7728, lr 2.17e-04 (patience 5/5)\n",
      "============================================================\n",
      "early stopping triggered! no improvement for 5 evaluations\n",
      "============================================================\n",
      "restored best model (val loss: 1.5349)\n",
      "============================================================\n",
      "training stopped early at step 5500\n",
      "best validation loss: 1.5349\n"
     ]
    }
   ],
   "source": [
    "# training loop with early stopping\n",
    "# training will stop early if validation loss doesn't improve for 'patience' evaluations\n",
    "print(f'training GPT for up to {max_iters} iterations')\n",
    "print(f'evaluating every {eval_interval} iterations')\n",
    "print(f'early stopping patience: {patience} evaluations ({patience * eval_interval} steps)')\n",
    "print('=' * 60)\n",
    "\n",
    "# track best validation loss for early stopping\n",
    "# best_val_loss: the lowest validation loss seen so far (starts at infinity)\n",
    "# best_model_state: a copy of the model weights at the best validation loss\n",
    "# patience_counter: counts how many evaluations since last improvement (starts at 0)\n",
    "# stopped_early: flag to indicate if training stopped before max_iters\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "stopped_early = False\n",
    "\n",
    "# iterate through all training iterations\n",
    "for iter in range(max_iters):\n",
    "    # evaluate loss periodically (every eval_interval steps or at the last step)\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        # compute average loss over eval_iters batches for both train and val\n",
    "        losses = estimate_loss(model)\n",
    "        # get current learning rate from scheduler for logging\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # check if this is the best model so far (lowest validation loss)\n",
    "        if losses['val'] < best_val_loss:\n",
    "            # new best model found!\n",
    "            best_val_loss = losses['val']\n",
    "            # save a copy of all model weights (clone to avoid reference issues)\n",
    "            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            # reset patience counter since we improved\n",
    "            patience_counter = 0\n",
    "            print(f'step {iter:5d}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}, lr {current_lr:.2e} *best*')\n",
    "        else:\n",
    "            # no improvement, increment patience counter\n",
    "            patience_counter += 1\n",
    "            print(f'step {iter:5d}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}, lr {current_lr:.2e} (patience {patience_counter}/{patience})')\n",
    "            \n",
    "            # check if we've run out of patience\n",
    "            if patience_counter >= patience:\n",
    "                print('=' * 60)\n",
    "                print(f'early stopping triggered! no improvement for {patience} evaluations')\n",
    "                stopped_early = True\n",
    "                break\n",
    "    \n",
    "    # get batch of training data\n",
    "    # randomly samples batch_size sequences of length block_size\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # forward pass: compute predictions and loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # backward pass: compute gradients\n",
    "    # set_to_none=True is more efficient than zero_grad()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping to prevent exploding gradients\n",
    "    # if the total gradient norm exceeds 1.0, scale it down\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    # update weights using computed gradients\n",
    "    optimizer.step()\n",
    "    # decay learning rate according to cosine schedule\n",
    "    scheduler.step()\n",
    "\n",
    "# restore best model weights to avoid using an overfit model\n",
    "# this ensures we use the model from the step with lowest validation loss\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print('=' * 60)\n",
    "    print(f'restored best model (val loss: {best_val_loss:.4f})')\n",
    "\n",
    "# display training completion message\n",
    "print('=' * 60)\n",
    "if stopped_early:\n",
    "    print(f'training stopped early at step {iter}')\n",
    "else:\n",
    "    print('training complete (reached max_iters)')\n",
    "print(f'best validation loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591aa2cb",
   "metadata": {},
   "source": [
    "## Step 12: Generate Text from the Trained Model\n",
    "\n",
    "Now we can use our trained model to generate new text! The model predicts one token at a time:\n",
    "\n",
    "1. Feed in a starting context (or just a newline)\n",
    "2. Model outputs probability distribution over next token\n",
    "3. Sample from the distribution\n",
    "4. Append new token to context\n",
    "5. Repeat until we have enough tokens\n",
    "\n",
    "This is called **autoregressive generation** - each new token depends on all previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "358d5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text from trained GPT:\n",
      "============================================================\n",
      "\n",
      "\n",
      "Friston’s vigers of the wordco insuation.”\n",
      "\n",
      "“The Cigar Misder mornian business.”\n",
      "\n",
      "“Burst a moment very mney.”\n",
      "\n",
      "“Yes, sir, Mr. Holmes. It was already that I will pisend before you to\n",
      "busine one which I must be so possible that never girl to be in that\n",
      "I might come up in my mind work before, you have must not have from my\n",
      "miles, and you chanced back too it. Was the key, that poor upon his\n",
      "wrong and reway from his drive, and he returned himself in the other. His\n",
      "formidation, has there in this vadi\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# generate text from trained model\n",
    "# start with a newline character (or any starting context)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print('generated text from trained GPT:')\n",
    "print('=' * 60)\n",
    "generated = m.generate(context, max_new_tokens=500)\n",
    "print(decode(generated[0].tolist()))\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363c29c",
   "metadata": {},
   "source": [
    "## Step 13: Save the Trained Model\n",
    "\n",
    "Training a model takes time and resources. We save the trained model so we can:\n",
    "- **Reuse it later** without retraining\n",
    "- **Share it** with others\n",
    "- **Deploy it** in applications\n",
    "\n",
    "**What we save in the checkpoint:**\n",
    "- `model_state_dict`: All learned weights and biases\n",
    "- Architecture hyperparameters: `vocab_size`, `n_embd`, `n_head`, `n_layer`, `block_size`, `dropout`\n",
    "- Tokenizer mappings: `chars`, `stoi`, `itos` (needed to encode/decode text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8611c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to gpt_model.pt\n",
      "\n",
      "checkpoint contains:\n",
      "   model_state_dict: 210 parameter tensors\n",
      "   vocab_size: 98\n",
      "   n_embd: 384\n",
      "   n_head: 6\n",
      "   n_layer: 6\n",
      "   block_size: 256\n",
      "   dropout: 0.35\n",
      "   chars: 98 unique characters\n",
      "   stoi: character to index mapping\n",
      "   itos: index to character mapping\n"
     ]
    }
   ],
   "source": [
    "# save the trained model\n",
    "# we save both the model state and hyperparameters needed for reconstruction\n",
    "\n",
    "# create a checkpoint dictionary with all necessary information\n",
    "checkpoint = {\n",
    "    # model weights and biases\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    \n",
    "    # hyperparameters needed to reconstruct the model\n",
    "    'vocab_size': vocab_size,\n",
    "    'n_embd': n_embd,\n",
    "    'n_head': n_head,\n",
    "    'n_layer': n_layer,\n",
    "    'block_size': block_size,\n",
    "    'dropout': dropout,\n",
    "    \n",
    "    # tokenizer mappings (needed for encode/decode)\n",
    "    'chars': chars,\n",
    "    'stoi': stoi,\n",
    "    'itos': itos,\n",
    "}\n",
    "\n",
    "# save to file\n",
    "model_path = 'gpt_model.pt'\n",
    "torch.save(checkpoint, model_path)\n",
    "\n",
    "# display save confirmation and checkpoint contents\n",
    "print(f'model saved to {model_path}')\n",
    "print()\n",
    "print('checkpoint contains:')\n",
    "print(f'   model_state_dict: {len(checkpoint[\"model_state_dict\"])} parameter tensors')\n",
    "print(f'   vocab_size: {vocab_size}')\n",
    "print(f'   n_embd: {n_embd}')\n",
    "print(f'   n_head: {n_head}')\n",
    "print(f'   n_layer: {n_layer}')\n",
    "print(f'   block_size: {block_size}')\n",
    "print(f'   dropout: {dropout}')\n",
    "print(f'   chars: {len(chars)} unique characters')\n",
    "print(f'   stoi: character to index mapping')\n",
    "print(f'   itos: index to character mapping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a40c7",
   "metadata": {},
   "source": [
    "## Step 14: Inference - Loading and Using the Trained Model\n",
    "\n",
    "Inference is the process of using a trained model to make predictions on new data.\n",
    "Unlike training, inference:\n",
    "- Does **not** compute gradients (faster and uses less memory)\n",
    "- Uses the model in **evaluation mode** (disables dropout)\n",
    "- Can process inputs of **variable length** (up to block_size)\n",
    "\n",
    "### Why Save and Load?\n",
    "1. **Avoid retraining**: Training takes time and compute resources\n",
    "2. **Deployment**: Use the model in production applications\n",
    "3. **Sharing**: Distribute trained models to others\n",
    "4. **Checkpointing**: Save progress during long training runs\n",
    "\n",
    "### The Inference Pipeline\n",
    "1. Load the checkpoint file\n",
    "2. Extract hyperparameters to reconstruct model architecture\n",
    "3. Create a new model with the same architecture\n",
    "4. Load the saved weights into the model\n",
    "5. Set model to evaluation mode\n",
    "6. Generate text from any starting prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d96f4c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference device: mps\n"
     ]
    }
   ],
   "source": [
    "# determine device for inference\n",
    "# this cell can be run independently after the model has been trained and saved\n",
    "# note: torch, nn, and F are already imported from earlier cells\n",
    "if torch.cuda.is_available():\n",
    "    inference_device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    inference_device = 'mps'\n",
    "else:\n",
    "    inference_device = 'cpu'\n",
    "\n",
    "# display the selected inference device\n",
    "print(f'inference device: {inference_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2eccb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded successfully\n",
      "\n",
      "hyperparameters from checkpoint:\n",
      "   vocab_size: 98\n",
      "   n_embd: 384\n",
      "   n_head: 6\n",
      "   n_layer: 6\n",
      "   block_size: 256\n",
      "   dropout: 0.35\n"
     ]
    }
   ],
   "source": [
    "# load the checkpoint\n",
    "checkpoint_path = 'gpt_model.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=inference_device, weights_only=False)\n",
    "\n",
    "# extract hyperparameters from checkpoint\n",
    "loaded_vocab_size = checkpoint['vocab_size']\n",
    "loaded_n_embd = checkpoint['n_embd']\n",
    "loaded_n_head = checkpoint['n_head']\n",
    "loaded_n_layer = checkpoint['n_layer']\n",
    "loaded_block_size = checkpoint['block_size']\n",
    "loaded_dropout = checkpoint['dropout']\n",
    "\n",
    "# extract tokenizer mappings\n",
    "loaded_chars = checkpoint['chars']\n",
    "loaded_stoi = checkpoint['stoi']\n",
    "loaded_itos = checkpoint['itos']\n",
    "\n",
    "# display checkpoint information\n",
    "print('checkpoint loaded successfully')\n",
    "print()\n",
    "print('hyperparameters from checkpoint:')\n",
    "print(f'   vocab_size: {loaded_vocab_size}')\n",
    "print(f'   n_embd: {loaded_n_embd}')\n",
    "print(f'   n_head: {loaded_n_head}')\n",
    "print(f'   n_layer: {loaded_n_layer}')\n",
    "print(f'   block_size: {loaded_block_size}')\n",
    "print(f'   dropout: {loaded_dropout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f4a451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer functions created\n",
      "\n",
      "testing tokenizer:\n",
      "   encode(\"hello\") = [63, 60, 67, 67, 70]\n",
      "   decode([63, 60, 67, 67, 70]) = \"hello\"\n"
     ]
    }
   ],
   "source": [
    "# define encode and decode functions using loaded tokenizer\n",
    "def inference_encode(s):\n",
    "    '''\n",
    "    Convert a string to a list of integers using loaded tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        s: input string to encode\n",
    "        \n",
    "    Returns:\n",
    "        list of integers representing each character\n",
    "    '''\n",
    "    # convert each character to its integer index\n",
    "    return [loaded_stoi[c] for c in s]\n",
    "\n",
    "def inference_decode(l):\n",
    "    '''\n",
    "    Convert a list of integers back to a string using loaded tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        l: list of integers to decode\n",
    "        \n",
    "    Returns:\n",
    "        string representation of the integers\n",
    "    '''\n",
    "    # convert each integer back to its character and join\n",
    "    return ''.join([loaded_itos[i] for i in l])\n",
    "\n",
    "# display confirmation and test tokenizer functions\n",
    "print('tokenizer functions created')\n",
    "print()\n",
    "print('testing tokenizer:')\n",
    "test_str = 'hello'\n",
    "encoded_test = inference_encode(test_str)\n",
    "decoded_test = inference_decode(encoded_test)\n",
    "print(f'   encode(\"{test_str}\") = {encoded_test}')\n",
    "print(f'   decode({encoded_test}) = \"{decoded_test}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61dd2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference model classes defined\n"
     ]
    }
   ],
   "source": [
    "# InferenceHead: one head of self-attention for inference\n",
    "class InferenceHead(nn.Module):\n",
    "    '''\n",
    "    Single head of self-attention for inference.\n",
    "    \n",
    "    Identical architecture to training Head class, but uses loaded hyperparameters\n",
    "    from the checkpoint file instead of global variables.\n",
    "    \n",
    "    Computes scaled dot-product attention:\n",
    "        Attention(Q, K, V) = softmax(Q @ K^T / sqrt(head_size)) @ V\n",
    "    \n",
    "    Args:\n",
    "        head_size: dimension of queries, keys, and values\n",
    "    \n",
    "    Attributes:\n",
    "        key: linear projection for keys (what I contain)\n",
    "        query: linear projection for queries (what I'm looking for)\n",
    "        value: linear projection for values (what I'll give)\n",
    "        tril: lower triangular mask for causal attention\n",
    "        dropout: dropout layer for regularization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        '''\n",
    "        Initialize the attention head.\n",
    "        \n",
    "        Args:\n",
    "            head_size: dimension of the attention head\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # key projection: (loaded_n_embd) -> (head_size)\n",
    "        self.key = nn.Linear(loaded_n_embd, head_size, bias=False)\n",
    "        \n",
    "        # query projection: (loaded_n_embd) -> (head_size)\n",
    "        self.query = nn.Linear(loaded_n_embd, head_size, bias=False)\n",
    "        \n",
    "        # value projection: (loaded_n_embd) -> (head_size)\n",
    "        self.value = nn.Linear(loaded_n_embd, head_size, bias=False)\n",
    "        \n",
    "        # register_buffer: not a parameter, but should be saved with model\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(loaded_block_size, loaded_block_size)))\n",
    "        \n",
    "        # type hint for tril buffer\n",
    "        self.tril: torch.Tensor\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(loaded_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the attention head.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "               B = batch size\n",
    "               T = sequence length\n",
    "               C = loaded_n_embd (embedding dimension)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, head_size)\n",
    "        '''\n",
    "        # get dimensions (only T is needed for masking)\n",
    "        _, T, _ = x.shape\n",
    "        \n",
    "        # compute keys and queries\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # compute attention scores with scaling\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        \n",
    "        # apply causal mask\n",
    "        tril = self.tril\n",
    "        wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # apply softmax and dropout\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # compute values and weighted aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        # return the attention output\n",
    "        return out\n",
    "\n",
    "\n",
    "# InferenceMultiHeadAttention: multiple heads of self-attention in parallel\n",
    "class InferenceMultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Multi-head self-attention for inference.\n",
    "    \n",
    "    Identical architecture to training MultiHeadAttention class, but uses loaded\n",
    "    hyperparameters from the checkpoint file instead of global variables.\n",
    "    \n",
    "    Runs multiple attention heads in parallel and concatenates the results.\n",
    "    Then projects back to the embedding dimension.\n",
    "    \n",
    "    Args:\n",
    "        num_heads: number of attention heads\n",
    "        head_size: dimension of each head\n",
    "    \n",
    "    Attributes:\n",
    "        heads: ModuleList of InferenceHead modules\n",
    "        proj: output projection back to loaded_n_embd\n",
    "        dropout: dropout layer for regularization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        '''\n",
    "        Initialize multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            num_heads: number of parallel attention heads\n",
    "            head_size: dimension of each attention head\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # create num_heads attention heads\n",
    "        self.heads = nn.ModuleList([InferenceHead(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "        # output projection: (num_heads * head_size) -> (loaded_n_embd)\n",
    "        self.proj = nn.Linear(head_size * num_heads, loaded_n_embd)\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(loaded_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, C)\n",
    "        '''\n",
    "        # run all heads in parallel and concatenate\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        # project back to embedding dimension and apply dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        \n",
    "        # return the multi-head attention output\n",
    "        return out\n",
    "\n",
    "\n",
    "# InferenceFeedForward: simple feed-forward network (per token)\n",
    "class InferenceFeedForward(nn.Module):\n",
    "    '''\n",
    "    Position-wise feed-forward network for inference.\n",
    "    \n",
    "    Identical architecture to training FeedForward class, but uses loaded\n",
    "    hyperparameters from the checkpoint file instead of global variables.\n",
    "    \n",
    "    Applied to each position independently and identically.\n",
    "    Expands to 4x the embedding dimension, applies GELU, then projects back.\n",
    "    \n",
    "    Args:\n",
    "        n_embd: embedding dimension\n",
    "    \n",
    "    Attributes:\n",
    "        net: Sequential network with linear, gelu, linear, dropout\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        '''\n",
    "        Initialize the feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            n_embd: embedding dimension (input and output size)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # build the feed-forward network\n",
    "        self.net = nn.Sequential(\n",
    "            # expand to 4x embedding dimension\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            # GELU non-linearity (smoother than ReLU, used in GPT-2/3)\n",
    "            nn.GELU(),\n",
    "            # project back to embedding dimension\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            # dropout for regularization\n",
    "            nn.Dropout(loaded_dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, C)\n",
    "        '''\n",
    "        # apply the feed-forward network\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# InferenceBlock: transformer block with communication followed by computation\n",
    "class InferenceBlock(nn.Module):\n",
    "    '''\n",
    "    Transformer block for inference: communication (attention) followed by computation (ffn).\n",
    "    \n",
    "    Identical architecture to training Block class, but uses loaded\n",
    "    hyperparameters from the checkpoint file instead of global variables.\n",
    "    \n",
    "    Uses pre-norm architecture where layer norm is applied before each sub-layer.\n",
    "    Residual connections allow gradients to flow directly through the network.\n",
    "    \n",
    "    Args:\n",
    "        n_embd: embedding dimension\n",
    "        n_head: number of attention heads\n",
    "    \n",
    "    Attributes:\n",
    "        sa: self-attention module (multi-head attention)\n",
    "        ffwd: feed-forward network\n",
    "        ln1: first layer normalization\n",
    "        ln2: second layer normalization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        '''\n",
    "        Initialize the transformer block.\n",
    "        \n",
    "        Args:\n",
    "            n_embd: embedding dimension\n",
    "            n_head: number of attention heads\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # calculate head size: n_embd must be divisible by n_head\n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        # self-attention layer (communication)\n",
    "        self.sa = InferenceMultiHeadAttention(n_head, head_size)\n",
    "        \n",
    "        # feed-forward layer (computation)\n",
    "        self.ffwd = InferenceFeedForward(n_embd)\n",
    "        \n",
    "        # layer normalizations (pre-norm architecture)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (B, T, C)\n",
    "        \n",
    "        Returns:\n",
    "            out: output tensor of shape (B, T, C)\n",
    "        '''\n",
    "        # self-attention with residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # feed-forward with residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        # return the transformed output\n",
    "        return x\n",
    "\n",
    "\n",
    "# InferenceGPT: the complete GPT model for inference\n",
    "class InferenceGPT(nn.Module):\n",
    "    '''\n",
    "    GPT Language Model for inference.\n",
    "    \n",
    "    Identical architecture to training GPTLanguageModel class, but uses loaded\n",
    "    hyperparameters from the checkpoint file instead of global variables.\n",
    "    \n",
    "    A decoder-only transformer that predicts the next token given previous tokens.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token embedding: convert token indices to vectors\n",
    "        2. Position embedding: add position information\n",
    "        3. Transformer blocks: stack of attention + ffn blocks\n",
    "        4. Final layer norm: normalize before output\n",
    "        5. Language model head: project to vocabulary size for predictions\n",
    "    \n",
    "    Attributes:\n",
    "        token_embedding_table: lookup table for token embeddings\n",
    "        position_embedding_table: lookup table for position embeddings\n",
    "        blocks: Sequential stack of transformer blocks\n",
    "        ln_f: final layer normalization\n",
    "        lm_head: linear projection to vocabulary size\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the GPT language model for inference.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding table: (loaded_vocab_size, loaded_n_embd)\n",
    "        self.token_embedding_table = nn.Embedding(loaded_vocab_size, loaded_n_embd)\n",
    "        \n",
    "        # position embedding table: (loaded_block_size, loaded_n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(loaded_block_size, loaded_n_embd)\n",
    "        \n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[InferenceBlock(loaded_n_embd, n_head=loaded_n_head) for _ in range(loaded_n_layer)])\n",
    "        \n",
    "        # final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(loaded_n_embd)\n",
    "        \n",
    "        # language model head: project from loaded_n_embd to loaded_vocab_size\n",
    "        self.lm_head = nn.Linear(loaded_n_embd, loaded_vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        Forward pass of the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            idx: input token indices of shape (B, T)\n",
    "            targets: target token indices of shape (B, T), optional\n",
    "        \n",
    "        Returns:\n",
    "            logits: unnormalized predictions of shape (B, T, loaded_vocab_size)\n",
    "            loss: cross-entropy loss if targets provided, else None\n",
    "        '''\n",
    "        # get sequence length\n",
    "        _, T = idx.shape\n",
    "        \n",
    "        # get token embeddings: (B, T) -> (B, T, loaded_n_embd)\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        \n",
    "        # get position embeddings: (T,) -> (T, loaded_n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=inference_device))\n",
    "        \n",
    "        # combine token and position embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # project to vocabulary\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape for cross entropy\n",
    "            _, _, C = logits.shape\n",
    "            logits_flat = logits.view(-1, C)\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        # return predictions and optional loss\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Given a context, generate max_new_tokens new tokens one at a time.\n",
    "        \n",
    "        Args:\n",
    "            idx: starting context of shape (B, T)\n",
    "            max_new_tokens: number of new tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            idx: extended sequence of shape (B, T + max_new_tokens)\n",
    "        '''\n",
    "        # generate one token at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop context to loaded_block_size\n",
    "            idx_cond = idx[:, -loaded_block_size:]\n",
    "            \n",
    "            # get predictions\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # focus on last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # append to running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        # return the extended sequence\n",
    "        return idx\n",
    "\n",
    "# display confirmation\n",
    "print('inference model classes defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd1fa03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded and ready for inference\n",
      "\n",
      "model device: mps\n",
      "model mode: evaluation (dropout disabled)\n",
      "max context length: 256 tokens\n"
     ]
    }
   ],
   "source": [
    "# create inference model and load saved weights\n",
    "inference_model = InferenceGPT()\n",
    "inference_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "inference_model = inference_model.to(inference_device)\n",
    "\n",
    "# set model to evaluation mode\n",
    "# this disables dropout for deterministic inference\n",
    "inference_model.eval()\n",
    "\n",
    "# display model status and configuration\n",
    "print('model loaded and ready for inference')\n",
    "print()\n",
    "print(f'model device: {inference_device}')\n",
    "print(f'model mode: evaluation (dropout disabled)')\n",
    "print(f'max context length: {loaded_block_size} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55829bc",
   "metadata": {},
   "source": [
    "### Generate Text w/ Variable Context Length\n",
    "\n",
    "You can provide any starting prompt (context) and the model will continue generating from there. The context can be any length from 1 character up to `block_size` characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "084cb53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \"The \"\n",
      "context length: 4 tokens\n",
      "tokens to generate: 500\n",
      "\n",
      "generated text:\n",
      "============================================================\n",
      "The panies he\n",
      "has easy, and Mr. James, we have done me on the morning. If you will be\n",
      "break if you from anything that I may still of making my absolutely commonplace easy\n",
      "at Briony Lodge, this is quite as to the furniture of the room above, this\n",
      "vacancy, which you will not remain the meantime of whom we wish to a\n",
      "requestion to open the struggle. There is one that the day which you are\n",
      "sure this morning end.”\n",
      "\n",
      "“And you will know how question. I did not best to be asticle enough.”\n",
      "\n",
      "“You know, I fancy,\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# generate text with variable context length\n",
    "# change these variables to customize generation\n",
    "\n",
    "# starting prompt (context) - can be any text from the training data's character set\n",
    "# the model will continue generating from this starting point\n",
    "prompt = 'The '\n",
    "\n",
    "# number of new tokens to generate\n",
    "num_tokens_to_generate = 500\n",
    "\n",
    "# validate prompt contains only known characters\n",
    "for char in prompt:\n",
    "    if char not in loaded_stoi:\n",
    "        print(f'warning: character \"{char}\" not in vocabulary, replacing with space')\n",
    "        prompt = prompt.replace(char, ' ')\n",
    "\n",
    "# encode the prompt\n",
    "context_tokens = inference_encode(prompt)\n",
    "context_length = len(context_tokens)\n",
    "\n",
    "# display prompt information\n",
    "print(f'prompt: \"{prompt}\"')\n",
    "print(f'context length: {context_length} tokens')\n",
    "print(f'tokens to generate: {num_tokens_to_generate}')\n",
    "print()\n",
    "\n",
    "# convert to tensor and move to device\n",
    "context_tensor = torch.tensor([context_tokens], dtype=torch.long, device=inference_device)\n",
    "\n",
    "# generate with no gradient computation (faster and uses less memory)\n",
    "with torch.no_grad():\n",
    "    generated_tokens = inference_model.generate(context_tensor, max_new_tokens=num_tokens_to_generate)\n",
    "\n",
    "# decode and display\n",
    "generated_text = inference_decode(generated_tokens[0].tolist())\n",
    "\n",
    "# display the generated text\n",
    "print('generated text:')\n",
    "print('=' * 60)\n",
    "print(generated_text)\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6599bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_text function defined\n",
      "\n",
      "usage:\n",
      "   generate_text(prompt=\"The \", num_tokens=200)\n",
      "   generate_text(prompt=\"\", num_tokens=500)  # start from scratch\n",
      "\n",
      "example:\n",
      "----------------------------------------\n",
      "What is your name?”\n",
      "\n",
      "“I should be be good for passional jest for thi\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reusable function for generating text with any prompt\n",
    "def generate_text(prompt='', num_tokens=200):\n",
    "    '''\n",
    "    Generate text from the trained model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: starting text (empty string starts from scratch)\n",
    "        num_tokens: number of new tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        generated text string\n",
    "    '''\n",
    "    # handle empty prompt\n",
    "    if prompt == '':\n",
    "        context_tensor = torch.zeros((1, 1), dtype=torch.long, device=inference_device)\n",
    "    else:\n",
    "        # validate and encode prompt\n",
    "        valid_prompt = ''\n",
    "        for char in prompt:\n",
    "            if char in loaded_stoi:\n",
    "                valid_prompt += char\n",
    "            else:\n",
    "                valid_prompt += ' '\n",
    "        context_tokens = inference_encode(valid_prompt)\n",
    "        context_tensor = torch.tensor([context_tokens], dtype=torch.long, device=inference_device)\n",
    "    \n",
    "    # generate with no gradients\n",
    "    with torch.no_grad():\n",
    "        generated = inference_model.generate(context_tensor, max_new_tokens=num_tokens)\n",
    "    \n",
    "    # decode and return the generated text\n",
    "    return inference_decode(generated[0].tolist())\n",
    "\n",
    "# display function usage and example\n",
    "print('generate_text function defined')\n",
    "print()\n",
    "print('usage:')\n",
    "print('   generate_text(prompt=\"The \", num_tokens=200)')\n",
    "print('   generate_text(prompt=\"\", num_tokens=500)  # start from scratch')\n",
    "print()\n",
    "print('example:')\n",
    "print('-' * 40)\n",
    "sample = generate_text(prompt='What is your name?', num_tokens=50)\n",
    "print(sample)\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ffa83",
   "metadata": {},
   "source": [
    "## Summary: What We Built\n",
    "\n",
    "We implemented a complete GPT language model from scratch with the following components:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| **Token Embedding** | Convert token indices to dense vectors |\n",
    "| **Position Embedding** | Add position information to tokens |\n",
    "| **Multi-Head Attention** | Tokens communicate with each other |\n",
    "| **Feed-Forward Network** | Process each token independently (with GELU activation) |\n",
    "| **Layer Normalization** | Stabilize training (pre-norm architecture) |\n",
    "| **Residual Connections** | Enable deep networks |\n",
    "| **Model Saving** | Save trained weights to disk |\n",
    "| **Inference Pipeline** | Load and use model for generation |\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Our Hyperparameters\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `n_embd` | 384 | Embedding dimension |\n",
    "| `n_head` | 6 | Number of attention heads |\n",
    "| `n_layer` | 6 | Number of transformer blocks |\n",
    "| `block_size` | 256 | Maximum context length |\n",
    "| `batch_size` | 64 | Training batch size |\n",
    "| `dropout` | 0.35 | Regularization |\n",
    "| `patience` | 5 | Early stopping patience (evaluations) |\n",
    "| `max_iters` | 10,000 | Maximum training iterations |\n",
    "| `learning_rate` | 5e-4 | Initial learning rate |\n",
    "\n",
    "### Training Techniques\n",
    "\n",
    "| Technique | Implementation | Purpose |\n",
    "|-----------|----------------|---------|\n",
    "| **AdamW Optimizer** | `weight_decay=0.15` | L2 regularization to prevent overfitting |\n",
    "| **Cosine LR Scheduler** | 5e-4 → 1e-5 | Smooth learning rate decay for better convergence |\n",
    "| **Gradient Clipping** | `max_norm=1.0` | Prevent exploding gradients |\n",
    "| **GELU Activation** | `nn.GELU()` | Smoother gradients than ReLU (used in GPT-2/3) |\n",
    "| **Early Stopping** | `patience=5` | Stop training when val loss stops improving |\n",
    "\n",
    "### GPU Acceleration Support\n",
    "\n",
    "| Platform | Device | How It's Used |\n",
    "|----------|--------|---------------|\n",
    "| Windows/Linux | NVIDIA CUDA | `torch.cuda.is_available()` |\n",
    "| Mac (M1/M2/M3) | Apple MPS | `torch.backends.mps.is_available()` |\n",
    "| Any | CPU | Fallback when no GPU available |\n",
    "\n",
    "### Model Persistence\n",
    "\n",
    "The trained model is saved with:\n",
    "- **model_state_dict**: All learned weights and biases\n",
    "- **Hyperparameters**: Architecture configuration\n",
    "- **Tokenizer**: Character mappings for encode/decode\n",
    "\n",
    "Congratulations! You have built a complete GPT from scratch with training, saving, and inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a5ee4b",
   "metadata": {},
   "source": [
    "## MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
